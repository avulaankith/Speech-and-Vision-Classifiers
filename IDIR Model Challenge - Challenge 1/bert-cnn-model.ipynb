{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T09:00:29.038892Z","iopub.status.busy":"2024-04-14T09:00:29.038505Z","iopub.status.idle":"2024-04-14T09:35:48.525503Z","shell.execute_reply":"2024-04-14T09:35:48.524730Z","shell.execute_reply.started":"2024-04-14T09:00:29.038860Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","\n","Epoch 1 / 5:\n","Batch 1012/1012 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.05, accuracy=0.875\n","\n","Evaluating...\n","Batch 113/113 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.04, accuracy=1.05\n","\n","Training Loss: 0.046\n","Validation Loss: 0.037\n","\n","\n","Training Accuracy: 0.878\n","Validation Accuracy: 0.918\n","\n","Epoch 2 / 5:\n","Batch 1012/1012 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.03, accuracy=1.05\n","\n","Evaluating...\n","Batch 113/113 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.05, accuracy=0.875\n","\n","Training Loss: 0.030\n","Validation Loss: 0.047\n","\n","\n","Training Accuracy: 0.943\n","Validation Accuracy: 0.918\n","\n","Epoch 3 / 5:\n","Batch 1012/1012 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.02, accuracy=1.05\n","\n","Evaluating...\n","Batch 113/113 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.08, accuracy=1.05\n","\n","Training Loss: 0.016\n","Validation Loss: 0.086\n","\n","\n","Training Accuracy: 0.972\n","Validation Accuracy: 0.909\n","\n","Epoch 4 / 5:\n","Batch 1012/1012 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=1.05\n","\n","Evaluating...\n","Batch 113/113 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.10, accuracy=1.05\n","\n","Training Loss: 0.007\n","Validation Loss: 0.103\n","\n","\n","Training Accuracy: 0.986\n","Validation Accuracy: 0.920\n","\n","Epoch 5 / 5:\n","Batch 1012/1012 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=1.05\n","\n","Evaluating...\n","Batch 113/113 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.12, accuracy=1.05\n","\n","Training Loss: 0.005\n","Validation Loss: 0.124\n","\n","\n","Training Accuracy: 0.992\n","Validation Accuracy: 0.917\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from transformers import BertTokenizer, BertModel\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","import gc\n","\n","# Load the dataset\n","df = pd.read_csv(\"/kaggle/input/checkworthiness/checkworthiness_labeled.csv\")\n","df_cleaned = df.dropna()  # Remove rows with any missing values\n","df = df_cleaned\n","\n","# Define BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize input texts\n","tokenized_texts = tokenizer(df['Text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n","\n","# Convert labels to tensor\n","label_map = {'Yes': 1, 'No': 0}  # Define mapping for categories to integers\n","labels = torch.tensor(df['Category'].map(label_map).tolist())\n","\n","class BertCNNClassifier(nn.Module):\n","    def __init__(self, num_classes, dropout_prob=0.1, bert_model_name='bert-base-uncased', cnn_out_channels=128, cnn_kernel_sizes=(2, 3, 4), dropout_p=0.1):\n","        super(BertCNNClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","        \n","        # CNN layers with batch normalization and dropout\n","        self.convs = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv1d(in_channels=self.bert.config.hidden_size, out_channels=cnn_out_channels, kernel_size=k),\n","                nn.BatchNorm1d(cnn_out_channels),\n","                nn.ReLU(),\n","                nn.Dropout(dropout_p)\n","            )\n","            for k in cnn_kernel_sizes\n","        ])\n","        \n","        # Fully connected layer with dropout\n","        self.fc = nn.Sequential(\n","            nn.Linear(len(cnn_kernel_sizes) * cnn_out_channels, num_classes),\n","            nn.Dropout(dropout_prob)\n","        )\n","        \n","    def forward(self, input_ids, attention_mask):\n","        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]  # Output of BERT last layer\n","        \n","        # Permute to have the shape (batch_size, hidden_size, sequence_length)\n","        bert_output = bert_output.permute(0, 2, 1)\n","        \n","        # Apply CNNs and pool over time\n","        conv_outputs = [conv(bert_output) for conv in self.convs]\n","        pooled_outputs = [torch.max(conv_output, dim=2)[0] for conv_output in conv_outputs]\n","        \n","        # Concatenate pooled outputs\n","        cat_output = torch.cat(pooled_outputs, dim=1)\n","        \n","        # Fully connected layer\n","        output = self.fc(cat_output)\n","        \n","        return output\n","\n","# Initialize the model and move it to GPU if available\n","num_classes = 2  # Assuming binary classification\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = BertCNNClassifier(num_classes=num_classes).to(device)\n","\n","# Split data into train and validation sets\n","train_texts, val_texts, train_labels, val_labels = train_test_split(tokenized_texts['input_ids'], labels, test_size=0.1, random_state=42)\n","train_masks, val_masks, _, _ = train_test_split(tokenized_texts['attention_mask'], tokenized_texts['attention_mask'], test_size=0.1, random_state=42)\n","\n","# Convert to PyTorch tensors and move to GPU\n","train_texts = train_texts.to(device)\n","val_texts = val_texts.to(device)\n","train_masks = train_masks.to(device)\n","val_masks = val_masks.to(device)\n","train_labels = train_labels.to(device)\n","val_labels = val_labels.to(device)\n","\n","# DataLoaders\n","batch_size = 8\n","train_dataset = TensorDataset(train_texts, train_masks, train_labels)\n","val_dataset = TensorDataset(val_texts, val_masks, val_labels)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Loss function and optimizer with weight decay\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n","\n","# Accuracy Functions\n","def get_accuracy_per_batch(oglabels, predlabels):\n","    correct = (predlabels == oglabels).sum().item()\n","    total = len(oglabels)\n","    accuracy = correct / total\n","    return accuracy\n","\n","def get_total_accuracy(acc_list):\n","    return sum(acc_list) / len(acc_list)\n","\n","# Training function\n","def train():\n","    model.train()\n","    total_loss, total_accuracy = 0, 0\n","    total_preds = []\n","    accuracy = 0\n","    total = len(train_loader)\n","    acc_list = []\n","    for i, batch in enumerate(train_loader):\n","        step = i+1\n","        percent = \"{0:.2f}\".format(100 * (step / float(total)))\n","        lossp = \"{0:.2f}\".format(total_loss / (total * batch_size))\n","        filledLength = int(100 * step // total)\n","        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n","        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n","        # Push the batch to GPU\n","        batch = [r.to(device) for r in batch]\n","        sent_id, mask, labels = batch\n","        del batch\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Clear previously calculated gradients\n","        optimizer.zero_grad()\n","        # Get model predictions for the current batch\n","        preds = model(sent_id.long(), mask)\n","        predicted_labels = torch.argmax(preds, dim=1)\n","        accuracy = get_accuracy_per_batch(labels, predicted_labels)\n","        acc_list.append(accuracy)\n","        # Compute the loss between actual and predicted values\n","        loss = criterion(preds, labels)\n","        # Add on to the total loss\n","        total_loss += float(loss.item())\n","        # Backward pass to calculate the gradients\n","        loss.backward()\n","        # Clip the gradients to prevent the exploding gradient problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        # Update parameters\n","        optimizer.step()\n","        # Model predictions are stored on GPU. So, push it to CPU and append the model predictions\n","        total_preds.append(preds.detach().cpu().numpy())\n","        \n","    # Compute the training loss of the epoch\n","    avg_loss = total_loss / (len(train_loader) * batch_size)\n","    # Average accuracy\n","    total_accuracy = get_total_accuracy(acc_list)\n","    # Reshape the predictions\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    return avg_loss, total_preds, total_accuracy\n","\n","# Evaluation function\n","def evaluate():\n","    print(\"\\n\\nEvaluating...\")\n","    model.eval()\n","    total_loss, total_accuracy = 0, 0\n","    accuracy = 0\n","    total_preds = []\n","    acc_list = []\n","    total = len(val_loader)\n","    for i, batch in enumerate(val_loader):\n","        step = i+1\n","        percent = \"{0:.2f}\".format(100 * (step / float(total)))\n","        lossp = \"{0:.2f}\".format(total_loss / (total * batch_size))\n","        filledLength = int(100 * step // total)\n","        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\n","        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}', end='')\n","        # Push the batch to GPU\n","        batch = [t.to(device) for t in batch]\n","        sent_id, mask, labels = batch\n","        del batch\n","        gc.collect()\n","        torch.cuda.empty_cache()\n","        # Deactivate autograd\n","        with torch.no_grad():\n","            # Model predictions\n","            preds = model(sent_id, mask)\n","            # Compute the validation loss between actual and predicted values\n","            loss = criterion(preds, labels)\n","            total_loss += float(loss.item())\n","            total_preds.append(preds.detach().cpu().numpy())\n","            predicted_labels = torch.argmax(preds, dim=1)\n","            accuracy = get_accuracy_per_batch(labels, predicted_labels)\n","            acc_list.append(accuracy)\n","    # Compute the validation loss of the epoch\n","    avg_loss = total_loss / (len(val_loader) * batch_size)\n","    # Average accuracy\n","    total_accuracy = get_total_accuracy(acc_list)\n","    # Reshape the predictions\n","    total_preds = np.concatenate(total_preds, axis=0)\n","    return avg_loss, total_preds, total_accuracy\n","\n","print(device)\n","\n","# Training and Validation loop\n","best_accuracy = 0.0  \n","best_model_state = None  \n","\n","# Define the number of epochs\n","epochs = 6\n","current = 1\n","\n","# Training and Validation loop\n","while current <= epochs:\n","    print(f'\\nEpoch {current} / {epochs}:')\n","\n","    # Train model\n","    train_loss, _, train_acc = train()\n","\n","    # Evaluate model\n","    valid_loss, _, valid_acc = evaluate()\n","\n","    # Check if the current epoch's accuracy is the best so far\n","    if valid_acc > best_accuracy:\n","        best_accuracy = valid_acc\n","        best_model_state = model.state_dict()\n","\n","    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\n","    print(f'Validation Loss: {valid_loss:.3f}')\n","    print(f'\\n\\nTraining Accuracy: {train_acc:.3f}')\n","    print(f'Validation Accuracy: {valid_acc:.3f}')\n","\n","    current = current + 1\n","\n","# Save the model with the best accuracy\n","if best_model_state is not None:\n","    torch.save(best_model_state, 'bert_cnn_model.pth')\n","\n","# Get predictions for test data\n","gc.collect()\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T09:56:12.822454Z","iopub.status.busy":"2024-04-14T09:56:12.821596Z","iopub.status.idle":"2024-04-14T09:56:17.918835Z","shell.execute_reply":"2024-04-14T09:56:17.917872Z","shell.execute_reply.started":"2024-04-14T09:56:12.822423Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Performance:\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.93      0.97      0.95       699\n","           1       0.86      0.75      0.80       201\n","\n","    accuracy                           0.92       900\n","   macro avg       0.90      0.86      0.88       900\n","weighted avg       0.92      0.92      0.92       900\n","\n","Accuracy: 0.9177777777777778\n"]}],"source":["from sklearn.metrics import classification_report, accuracy_score\n","\n","# Load the best model for evaluation\n","if best_model_state is not None:\n","    model.load_state_dict(best_model_state)\n","\n","# Evaluate the model\n","model.eval()\n","with torch.no_grad():\n","    preds = model(val_texts.to(device), val_masks.to(device))\n","    preds = preds.cpu().numpy()\n","\n","print(\"Performance:\")\n","# Model's performance\n","preds = np.argmax(preds, axis=1)\n","print('Classification Report:')\n","print(classification_report(val_labels.cpu(), preds))\n","\n","print(\"Accuracy:\", accuracy_score(val_labels.cpu(), preds))\n"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T10:05:23.076694Z","iopub.status.busy":"2024-04-14T10:05:23.076035Z","iopub.status.idle":"2024-04-14T10:05:29.241426Z","shell.execute_reply":"2024-04-14T10:05:29.240346Z","shell.execute_reply.started":"2024-04-14T10:05:23.076661Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 0 0 ... 0 0 0]\n","1467\n","['No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'Yes', 'No', 'Yes', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'No', 'Yes', 'No', 'No', 'No']\n","1467\n","         ID Category\n","0       131       No\n","1       219       No\n","2       384       No\n","3       411       No\n","4       432       No\n","...     ...      ...\n","1462  42558       No\n","1463  42569      Yes\n","1464  42571       No\n","1465  42580       No\n","1466  42585       No\n","\n","[1467 rows x 2 columns]\n"]}],"source":["test_df = pd.read_csv(\"/kaggle/input/checkworthiness/checkworthiness_leaderboard.csv\")\n","\n","tokenized_texts = tokenizer(test_df['Text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n","\n","test_texts, test_masks = tokenized_texts['input_ids'], tokenized_texts['attention_mask']\n","test_texts = test_texts.to(device)\n","test_masks = test_masks.to(device)\n","\n","ids = df['ID'].tolist()\n","\n","# Load the best model for evaluation\n","if best_model_state is not None:\n","    model.load_state_dict(best_model_state)\n","\n","with torch.no_grad():\n","    preds = model(test_texts, test_masks)\n","    preds = preds.detach().cpu().numpy()\n","preds = np.argmax(preds, axis=1)\n","\n","print(preds)\n","# Convert predictions to labels\n","labels = []\n","for i in range(len(preds)):\n","    if preds[i] > 0.5:\n","        labels.append(\"Yes\")\n","    else:\n","        labels.append(\"No\")\n","# labels = ['Yes' if pred > 0.5 else 'No' for pred in preds]\n","\n","print(len(labels))\n","\n","print(labels)\n","print(len(ids))\n","\n","# Ensure that both arrays have the same length\n","min_len = min(len(ids), len(labels))\n","ids = ids[:min_len]\n","labels = labels[:min_len]\n","\n","\n","\n","\n","# Create a DataFrame with IDs and predictions\n","result_df = pd.DataFrame({'ID': ids, 'Category': labels})\n","\n","# Merge with the test_df on ID column\n","# final_result_df = pd.merge(test_df, result_df, on='ID')\n","\n","# Print the final DataFrame\n","print(result_df)\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T10:05:41.006197Z","iopub.status.busy":"2024-04-14T10:05:41.005833Z","iopub.status.idle":"2024-04-14T10:05:41.014780Z","shell.execute_reply":"2024-04-14T10:05:41.013823Z","shell.execute_reply.started":"2024-04-14T10:05:41.006166Z"},"trusted":true},"outputs":[],"source":["result_df.to_csv(\"/kaggle/working/submission.csv\",index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-14T08:53:55.989690Z","iopub.status.busy":"2024-04-14T08:53:55.989336Z","iopub.status.idle":"2024-04-14T08:56:10.873965Z","shell.execute_reply":"2024-04-14T08:56:10.872529Z","shell.execute_reply.started":"2024-04-14T08:53:55.989662Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from transformers import BertTokenizer, BertModel\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from tqdm import tqdm\n","\n","# Load the dataset\n","df = pd.read_csv(\"/kaggle/input/checkworthiness/checkworthiness_labeled.csv\")\n","df_cleaned = df.dropna()  # Remove rows with any missing values\n","df = df_cleaned\n","\n","# Define BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Tokenize input texts\n","tokenized_texts = tokenizer(df['Text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n","\n","# Convert labels to tensor\n","label_map = {'Yes': 1, 'No': 0}  # Define mapping for categories to integers\n","labels = torch.tensor(df['Category'].map(label_map).tolist())\n","\n","class BertCNNClassifier(nn.Module):\n","    def __init__(self, num_classes, dropout_prob=0.1, bert_model_name='bert-base-uncased', cnn_out_channels=128, cnn_kernel_sizes=(2, 3, 4), dropout_p=0.1):\n","        super(BertCNNClassifier, self).__init__()\n","        self.bert = BertModel.from_pretrained(bert_model_name)\n","        \n","        # CNN layers with batch normalization and dropout\n","        self.convs = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Conv1d(in_channels=self.bert.config.hidden_size, out_channels=cnn_out_channels, kernel_size=k),\n","                nn.BatchNorm1d(cnn_out_channels),\n","                nn.ReLU(),\n","                nn.Dropout(dropout_p)\n","            )\n","            for k in cnn_kernel_sizes\n","        ])\n","        \n","        # Fully connected layer with dropout\n","        self.fc = nn.Sequential(\n","            nn.Linear(len(cnn_kernel_sizes) * cnn_out_channels, num_classes),\n","            nn.Dropout(dropout_prob)\n","        )\n","        \n","    def forward(self, input_ids, attention_mask):\n","        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)[0]  # Output of BERT last layer\n","        \n","        # Permute to have the shape (batch_size, hidden_size, sequence_length)\n","        bert_output = bert_output.permute(0, 2, 1)\n","        \n","        # Apply CNNs and pool over time\n","        conv_outputs = [conv(bert_output) for conv in self.convs]\n","        pooled_outputs = [torch.max(conv_output, dim=2)[0] for conv_output in conv_outputs]\n","        \n","        # Concatenate pooled outputs\n","        cat_output = torch.cat(pooled_outputs, dim=1)\n","        \n","        # Fully connected layer\n","        output = self.fc(cat_output)\n","        \n","        return output\n","\n","# Initialize the model and move it to GPU if available\n","num_classes = 2  # Assuming binary classification\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = BertCNNClassifier(num_classes=num_classes).to(device)\n","\n","# Split data into train and validation sets\n","train_texts, val_texts, train_labels, val_labels = train_test_split(tokenized_texts['input_ids'], labels, test_size=0.1, random_state=42)\n","train_masks, val_masks, _, _ = train_test_split(tokenized_texts['attention_mask'], tokenized_texts['attention_mask'], test_size=0.1, random_state=42)\n","\n","# Convert to PyTorch tensors and move to GPU\n","train_texts = train_texts.to(device)\n","val_texts = val_texts.to(device)\n","train_masks = train_masks.to(device)\n","val_masks = val_masks.to(device)\n","train_labels = train_labels.to(device)\n","val_labels = val_labels.to(device)\n","\n","# DataLoaders\n","batch_size = 8\n","train_dataset = TensorDataset(train_texts, train_masks, train_labels)\n","val_dataset = TensorDataset(val_texts, val_masks, val_labels)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Loss function and optimizer with weight decay\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n","\n","# Training loop\n","num_epochs = 3\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    tqdm_train_loader = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n","    for batch in tqdm_train_loader:\n","        input_ids, attention_mask, labels = [item.to(device) for item in batch]  # Move data to GPU\n","        optimizer.zero_grad()\n","        logits = model(input_ids, attention_mask)\n","        loss = criterion(logits, labels)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        tqdm_train_loader.set_postfix({'loss': running_loss / len(tqdm_train_loader)})\n","    epoch_loss = running_loss / len(train_loader)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss}\")\n","\n","    # Validation loop\n","    model.eval()\n","    val_running_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids, attention_mask, labels = [item.to(device) for item in batch]  # Move data to GPU\n","            logits = model(input_ids, attention_mask)\n","            loss = criterion(logits, labels)\n","            val_running_loss += loss.item()\n","            _, predicted = torch.max(logits, 1)\n","            val_correct += (predicted == labels).sum().item()\n","            val_total += labels.size(0)\n","        val_accuracy = val_correct / val_total\n","        val_epoch_loss = val_running_loss / len(val_loader)\n","        print(f\"Validation Loss: {val_epoch_loss}, Accuracy: {val_accuracy}\")\n","\n","# Testing\n","# Assuming you have a separate test set, follow a similar procedure as the validation loop\n","# Evaluate the model on the test set using accuracy or other appropriate metrics\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_df = pd.read_csv(\"/kaggle/input/checkworthiness/checkworthiness_leaderboard.csv\")\n","\n","tokenized_texts = tokenizer(test_df['Text'].tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n","\n","test_texts, test_masks = tokenized_texts['input_ids'], tokenized_texts['attention_mask']\n","test_texts = test_texts.to(device)\n","test_masks = test_mask.to(device)\n","\n","\n","\n","# DataLoaders\n","batch_size = 8\n","train_dataset = TensorDataset(train_texts, train_masks, train_labels)\n","val_dataset = TensorDataset(val_texts, val_masks, val_labels)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4793895,"sourceId":8114511,"sourceType":"datasetVersion"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
