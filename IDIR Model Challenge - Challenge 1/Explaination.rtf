{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 .SFNS-Regular_wdth_opsz110000_GRAD_wght2580000;\f1\fnil\fcharset0 HelveticaNeue;\f2\fnil\fcharset0 Monaco;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red255\green255\blue255;\red232\green232\blue231;
}
{\*\expandedcolortbl;;\cssrgb\c0\c1\c1;\cssrgb\c100000\c100000\c99985\c0;\cssrgb\c92753\c92753\c92521;
}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid201\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid202\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa120\partightenfactor0

\f0\b\fs30 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 1. Data Loading and Preprocessing:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Load the dataset from a CSV file.\
\ls1\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Clean the dataset by removing rows with missing values.\
\ls1\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Tokenize the input texts using the BERT tokenizer.\
\ls1\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Convert the labels to tensors.\
\pard\pardeftab720\sa120\partightenfactor0

\f0\b\fs30 \cf2 \cb3 \strokec4 2. Model Definition:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Define a custom neural network model called 
\f2\fs28 \cf2 \cb3 \strokec4 BertCNNClassifier
\f1\fs32 \cf2 \cb3 \strokec4 .\
\ls2\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 The model architecture combines BERT embeddings with a convolutional neural network (CNN).\
\ls2\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 BERT embeddings are obtained from the pre-trained BERT model.\
\ls2\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 CNN layers are applied to capture features from the BERT embeddings.\
\ls2\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 The final layer is a fully connected layer for classification.\
\pard\pardeftab720\sa120\partightenfactor0

\f0\b\fs30 \cf2 \cb3 \strokec4 3. Model Training:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Split the data into training and validation sets.\
\ls3\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Define the loss function (CrossEntropyLoss) and optimizer (Adam) with weight decay.\
\ls3\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Train the model for a specified number of epochs.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls3\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 In each epoch:\
\ls3\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Iterate over the training data in batches.\
\ls3\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Forward pass: Compute the output predictions using the model.\
\ls3\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Compute the loss between the predictions and the actual labels.\
\ls3\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Backpropagate the gradients and update the model parameters.\
\ls3\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Monitor and record the training loss.\
\pard\tx940\tx1440\pardeftab720\li1440\fi-1440\partightenfactor0
\ls3\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Evaluate the model on the validation set after each epoch:\
\ls3\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Iterate over the validation data in batches.\
\ls3\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Forward pass: Compute the output predictions using the model.\
\ls3\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Compute the validation loss.\
\ls3\ilvl1\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Monitor and record the validation loss.\
\pard\pardeftab720\sa120\partightenfactor0

\f0\b\fs30 \cf2 \cb3 \strokec4 4. Model Evaluation and Saving:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0
\f1\b0\fs32 \cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Save the model state with the best validation accuracy.\
\ls4\ilvl0\cf2 \cb3 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 {\listtext	\uc0\u8226 	}\cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec4 Optionally, you can load the saved model state to make predictions on new data.\
\pard\pardeftab720\sa120\partightenfactor0

\f0\b\fs30 \cf2 \cb3 \strokec4 Summary:\
\pard\pardeftab720\partightenfactor0

\f1\b0\fs32 \cf2 \cb3 \strokec4 This code implements a text classification model that combines BERT embeddings with a CNN architecture. It trains the model on a labeled dataset, monitors the training progress, and evaluates its performance on a validation set. Finally, it saves the best-performing model for future use.\
}